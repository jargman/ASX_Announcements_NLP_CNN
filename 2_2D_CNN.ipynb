{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8df88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import scikitplot as skplt\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "#import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc\n",
    "#!pip install pycorenlp\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('https://calm-panther-70.loca.lt')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "#!pip install wget\n",
    "import wget\n",
    "import gzip\n",
    "\n",
    "# url = \"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\"\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23409568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense\n",
    "from keras import backend as K \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_text\n",
    "\n",
    "\n",
    "#### setting all seeds for reproducible results\n",
    "seed_value= 420\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "merged_pdf_df = pd.read_csv ('merged_pdf_df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2de37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=1500\n",
    "\n",
    "sentences = merged_pdf_df.clean_content\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ae8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = wv\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d35e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e12bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_index.items(), columns=['word', 'value'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = embedding_matrix\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(words['word'])[1:300]\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e1d0e",
   "metadata": {},
   "source": [
    "## Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above three are from source\n",
    "# OR run this one\n",
    "rs = 420\n",
    "maxlen = 500 #state how long the sequences should be - anything longer that this gets cut\n",
    "word_num = 500\n",
    "#embedding_dim = 50\n",
    "embedding_dim = 128\n",
    "x_y_xplit = 0.25\n",
    "\n",
    "\n",
    "train_dataset = merged_pdf_df.sample(frac=0.8,random_state=50) #random state is a seed value\n",
    "test_dataset  = merged_pdf_df.drop(train_dataset.index)\n",
    "\n",
    "X_test_text = list(test_dataset['clean_text_tok'])\n",
    "Y_test = list(test_dataset['pnn_class'])\n",
    "X_train_text = list(train_dataset['clean_text_tok'])\n",
    "Y_train = list(train_dataset['pnn_class'])\n",
    "\n",
    "unique_classes = list(set(Y_train))\n",
    "target_classes = [\"UP\", \"DOWN\", \"SAME\"]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labenc = LabelEncoder()\n",
    "Y_train = labenc.fit_transform(Y_train)\n",
    "Y_test = labenc.fit_transform(Y_test)\n",
    "\n",
    "## Subtracted 1 from labels to bring range from 1-4 to 0-3\n",
    "#Y_train, Y_test = np.array(Y_train), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=word_num)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db87098",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) #post padding so 0s get added to the end, not start\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_and_acc(history):\n",
    "    #train_loss = history.history[\"loss\"]\n",
    "    #train_acc = history.history[\"accuracy\"]\n",
    "    #val_loss = history.history[\"val_loss\"]\n",
    "    #val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "    history_dict = history.history\n",
    "    print(history_dict.keys())\n",
    "\n",
    "    train_acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    train_loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']    \n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(range(len(train_loss)), train_loss, label=\"Train Loss\");\n",
    "    ax.plot(range(len(val_loss)), val_loss, label=\"Validation Loss\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\");\n",
    "    plt.title(\"Train Loss vs Validation Loss\");\n",
    "    plt.legend(loc=\"best\");\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(range(len(train_acc)), train_acc, label=\"Train Accuracy\");\n",
    "    ax.plot(range(len(val_acc)), val_acc, label=\"Validation Accuracy\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\");\n",
    "    plt.title(\"Train Accuracy vs Validation Accuracy\");\n",
    "    plt.legend(loc=\"best\");  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb9cb6",
   "metadata": {},
   "source": [
    "### Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b74af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "Y_train_cat = to_categorical(Y_train, 3)\n",
    "Y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "#unique_classes = list(set(Y_train))\n",
    "target_classes = [\"UP\", \"DOWN\", \"SAME\"]\n",
    "\n",
    "inputs    = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape   = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(len(target_classes), activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"auto\",patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, Y_train_cat, batch_size=500, epochs=16\n",
    "                    , validation_data=(X_test, Y_test_cat),callbacks=[es])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=True)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "gc.collect()\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted\n",
    "#model.fit(X_train_inp, y_train_inp, batch_size=1000, epochs=8, verbose=1, validation_data=(X_val_inp, y_val_inp))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c61ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode='auto',patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, Y_train_cat,\n",
    "                    epochs=32,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, Y_test_cat),\n",
    "                    batch_size=400, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n",
    "\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"2D CNN Confusion Matrix\",\n",
    "                                    cmap=\"Reds\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n",
    "\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"2D CNN Confusion Matrix\",\n",
    "                                    cmap=\"Reds\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc(history)  \n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Reds\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "gc.collect()\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted\n",
    "#model.fit(X_train_inp, y_train_inp, batch_size=1000, epochs=8, verbose=1, validation_data=(X_val_inp, y_val_inp))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca341e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train_inp, y_train_inp, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_val_inp, y_val_inp, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb03225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense\n",
    "from keras import backend as K \n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd492dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train_inp)\n",
    "test_preds = model.predict(X_val_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(classification_report(y_val_inp, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64209cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_and_acc(history):\n",
    "    train_loss = history.history[\"loss\"]\n",
    "    train_acc = history.history[\"acc\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    val_acc = history.history[\"val_acc\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(range(len(train_loss)), train_loss, label=\"Train Loss\");\n",
    "    ax.plot(range(len(val_loss)), val_loss, label=\"Validation Loss\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\");\n",
    "    plt.title(\"Train Loss vs Validation Loss\");\n",
    "    plt.legend(loc=\"best\");\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(range(len(train_acc)), train_acc, label=\"Train Accuracy\");\n",
    "    ax.plot(range(len(val_acc)), val_acc, label=\"Validation Accuracy\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\");\n",
    "    plt.title(\"Train Accuracy vs Validation Accuracy\");\n",
    "    plt.legend(loc=\"best\");\n",
    "\n",
    "plot_loss_and_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420bdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa7d312",
   "metadata": {},
   "source": [
    "### NOW DO LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n",
    "\n",
    "def make_predictions(X_batch_text):\n",
    "    X_batch = pad_sequences(tokenizer.texts_to_sequences(X_batch_text), maxlen=500, padding=\"post\", truncating=\"post\", value=0)\n",
    "    preds = model.predict(X_batch)\n",
    "    return preds\n",
    "\n",
    "def lime_run(i):\n",
    "    rng = np.random.RandomState(i)\n",
    "    idx = rng.randint(1, len(X_test))\n",
    "\n",
    "    print(\"Prediction : \", target_classes[model.predict(X_test[idx:idx+1]).argmax(axis=-1)[0]])\n",
    "    print(\"Actual :     \", target_classes[Y_test[idx]])\n",
    "\n",
    "    explanation = explainer.explain_instance(X_test_text[idx], classifier_fn=make_predictions, labels=Y_test[idx:idx+1], num_features=15)\n",
    "    explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1bf50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(400,450):\n",
    "    lime_run(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98491afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
