{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd181e2b",
   "metadata": {},
   "source": [
    "# 1D CNN USING GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tika import parser # pip install tika\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "#import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db80f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "merged_pdf_df = pd.read_csv (r'merged_pdf_df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above three are from source\n",
    "# OR run this one\n",
    "rs = 420\n",
    "maxlen = 500 #state how long the sequences should be - anything longer that this gets cut\n",
    "word_num = 500\n",
    "#embedding_dim = 50\n",
    "embedding_dim = 128\n",
    "x_y_xplit = 0.25\n",
    "\n",
    "\n",
    "train_dataset = merged_pdf_df.sample(frac=0.8,random_state=50) #random state is a seed value\n",
    "test_dataset  = merged_pdf_df.drop(train_dataset.index)\n",
    "\n",
    "X_test_text = list(test_dataset['clean_text_tok'])\n",
    "Y_test = list(test_dataset['pnn_class'])\n",
    "X_train_text = list(train_dataset['clean_text_tok'])\n",
    "Y_train = list(train_dataset['pnn_class'])\n",
    "\n",
    "unique_classes = list(set(Y_train))\n",
    "target_classes = [\"UP\", \"DOWN\", \"SAME\"]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labenc = LabelEncoder()\n",
    "Y_train = labenc.fit_transform(Y_train)\n",
    "Y_test = labenc.fit_transform(Y_test)\n",
    "\n",
    "## Subtracted 1 from labels to bring range from 1-4 to 0-3\n",
    "#Y_train, Y_test = np.array(Y_train), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=word_num)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) #post padding so 0s get added to the end, not start\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051203e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "#this must equal 50 because glove file is 50 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            print(word)\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.50d.txt'\n",
    "embedding_matrix = create_embedding_matrix(glove_file,tokenizer.word_index, embedding_dim )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size\n",
    "#51.9% of words in content are covered by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae177e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense\n",
    "from keras import backend as K \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_text\n",
    "\n",
    "\n",
    "#### setting all seeds for reproducible results\n",
    "seed_value= 420\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_and_acc(history):\n",
    "    train_loss = history.history[\"loss\"]\n",
    "    train_acc = history.history[\"accuracy\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(range(len(train_loss)), train_loss, label=\"Train Loss\");\n",
    "    ax.plot(range(len(val_loss)), val_loss, label=\"Validation Loss\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\");\n",
    "    plt.title(\"Train Loss vs Validation Loss\");\n",
    "    plt.legend(loc=\"best\");\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(range(len(train_acc)), train_acc, label=\"Train Accuracy\");\n",
    "    ax.plot(range(len(val_acc)), val_acc, label=\"Validation Accuracy\");\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\");\n",
    "    plt.title(\"Train Accuracy vs Validation Accuracy\");\n",
    "    plt.legend(loc=\"best\");  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23784dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbf00e",
   "metadata": {},
   "source": [
    "### THIS USES CATEGORICAL CROSSENTROPY \n",
    "### IN ORDER TO DO SO, WE NEED TO CONVERT THE HOTENCODED LABELS\n",
    "### HENCE WHY THIS IS IN A DIFFERENT SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af539ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### setting all seeds for reproducible results\n",
    "seed_value= 420\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "################################################\n",
    "\n",
    "def conv1d_func(embed_len,filt,kern,pad,act,epoch,btch_siz,esmon,esmod,espat):\n",
    "\n",
    "    Y_train_cat = to_categorical(Y_train, 3)\n",
    "    Y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "    embed_len = embed_len\n",
    "\n",
    "    inputs = Input(shape=(word_num, ))\n",
    "    embeddings_layer = Embedding(input_dim=vocab_size\n",
    "                                 ,weights=[embedding_matrix]\n",
    "                                 ,output_dim=embed_len\n",
    "                                 ,input_length=maxlen)\n",
    "    conv = Conv1D(filt, kern, padding=pad) ## Channels last\n",
    "    dense = Dense(len(target_classes), activation=act)\n",
    "\n",
    " \n",
    "    \n",
    "    x = embeddings_layer(inputs)\n",
    "    x = conv(x)\n",
    "    x = tensorflow.reduce_max(x, axis=1)\n",
    "    output = dense(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor=esmon, mode=esmod,patience=espat, verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, Y_train_cat,\n",
    "                        epochs=epoch,\n",
    "                        verbose=False,\n",
    "                        validation_data=(X_test, Y_test_cat),\n",
    "                        batch_size=btch_siz, \n",
    "                        callbacks=[es])\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=True)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=True)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "\n",
    "    #========================\n",
    "    plot_loss_and_acc(history)    \n",
    "    #========================\n",
    "\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "    print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "    print(\"\\nClassification Report : \")\n",
    "    print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                        normalize=False,\n",
    "                                        title=\"Confusion Matrix\",\n",
    "                                        cmap=\"Greens\",\n",
    "                                        hide_zeros=True,\n",
    "                                        figsize=(8,8)\n",
    "                                        );\n",
    "    plt.xticks(rotation=90);\n",
    "    \n",
    "    K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41630de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_func(embed_len=50,filt=80,kern=40,pad=\"valid\",act=\"softmax\",epoch=20,btch_siz=500,esmon=\"val_loss\",esmod=\"auto\",espat=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ae96e",
   "metadata": {},
   "source": [
    "### CREATING A SECOND CNN FUNCTION - TWO LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical crossentropy requires - surprise surprise - categorical variables\n",
    "#winner winner\n",
    "Y_train_cat = to_categorical(Y_train, 3)\n",
    "Y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size\n",
    "                                 ,weights=[embedding_matrix]\n",
    "                                 ,output_dim=50\n",
    "                                 ,input_length=maxlen\n",
    "                                 ,trainable=True))\n",
    "\n",
    "model.add(layers.Conv1D(50, 60, activation='softmax'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dense(len(target_classes), activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode='auto',patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, Y_train_cat,\n",
    "                    epochs=32,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, Y_test_cat),\n",
    "                    batch_size=250, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n",
    "\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n",
    "\n",
    "explainer\n",
    "\n",
    "def make_predictions(X_batch_text):\n",
    "    X_batch = pad_sequences(tokenizer.texts_to_sequences(X_batch_text), maxlen=500, padding=\"post\", truncating=\"post\", value=0)\n",
    "    preds = model.predict(X_batch)\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a00c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(30)\n",
    "idx = rng.randint(1, len(X_test))\n",
    "\n",
    "print(\"Prediction : \", target_classes[model.predict(X_test[idx:idx+1]).argmax(axis=-1)[0]])\n",
    "print(\"Actual :     \", target_classes[Y_test[idx]])\n",
    "\n",
    "explanation = explainer.explain_instance(X_test_text[idx], classifier_fn=make_predictions, labels=Y_test[idx:idx+1], num_features=15)\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb683ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n",
    "\n",
    "def make_predictions(X_batch_text):\n",
    "    X_batch = pad_sequences(tokenizer.texts_to_sequences(X_batch_text), maxlen=500, padding=\"post\", truncating=\"post\", value=0)\n",
    "    preds = model.predict(X_batch)\n",
    "    return preds\n",
    "\n",
    "def lime_run(i):\n",
    "    rng = np.random.RandomState(i)\n",
    "    idx = rng.randint(1, len(X_test))\n",
    "\n",
    "    print(\"Prediction : \", target_classes[model.predict(X_test[idx:idx+1]).argmax(axis=-1)[0]])\n",
    "    print(\"Actual :     \", target_classes[Y_test[idx]])\n",
    "\n",
    "    explanation = explainer.explain_instance(X_test_text[idx], classifier_fn=make_predictions, labels=Y_test[idx:idx+1], num_features=15)\n",
    "    explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0d880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(400,450):\n",
    "    lime_run(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc87747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#categorical crossentropy requires - surprise surprise - categorical variables\n",
    "#winner winner\n",
    "Y_train_cat = to_categorical(Y_train, 3)\n",
    "Y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size\n",
    "                                 ,weights=[embedding_matrix]\n",
    "                                 ,output_dim=50\n",
    "                                 ,input_length=maxlen\n",
    "                                 ,trainable=True))\n",
    "\n",
    "model.add(layers.Conv1D(80, 40, activation='softmax'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dense(len(target_classes), activation=\"tanh\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode='auto',patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, Y_train_cat,\n",
    "                    epochs=32,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, Y_test_cat),\n",
    "                    batch_size=250, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n",
    "\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a7da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#categorical crossentropy requires - surprise surprise - categorical variables\n",
    "Y_train_cat = to_categorical(Y_train, 3)\n",
    "Y_test_cat = to_categorical(Y_test, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size\n",
    "                                 ,weights=[embedding_matrix]\n",
    "                                 ,output_dim=50\n",
    "                                 ,input_length=maxlen\n",
    "                                 ,trainable=True))\n",
    "\n",
    "model.add(layers.Conv1D(30, 80, activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dense(len(target_classes), activation=\"tanh\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode='auto',patience=3, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, Y_train_cat,\n",
    "                    epochs=32,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, Y_test_cat),\n",
    "                    batch_size=250, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "loss, accuracy = model.evaluate(X_train, Y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_loss_and_acc(history)  \n",
    "\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy : {}\".format(accuracy_score(Y_train, np.argmax(train_preds, axis=1))))\n",
    "print(\"Test  Accuracy : {}\".format(accuracy_score(Y_test, np.argmax(test_preds, axis=1))))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, np.argmax(test_preds, axis=1), target_names=target_classes))\n",
    "\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test], [target_classes[i] for i in np.argmax(test_preds, axis=1)],\n",
    "                                    normalize=False,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Greens\",\n",
    "                                    hide_zeros=True,\n",
    "                                    figsize=(8,8)\n",
    "                                    );\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "K.clear_session() #this clears the session to forget previous model executions and become overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180b5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
