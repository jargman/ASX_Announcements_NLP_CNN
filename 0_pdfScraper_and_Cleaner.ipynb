{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2211f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tika import parser # pip install tika\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "\n",
    "from pathlib import Path #used for checking existence of csvs/pdfs later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to look up company on yahoo, take ticker code and concatenate \".ax\" in the search bar\n",
    "#https://au.finance.yahoo.com/quote/CBA.AX/history?p=CBA.AX&.tsrc=fin-srch\n",
    "#https://au.finance.yahoo.com/quote/LKE.AX/history?p=LKE.AX&.tsrc=fin-srch\n",
    "#https://au.finance.yahoo.com/quote/LKE.AX/history\n",
    "\n",
    "#this takes to announcement page on commsec\n",
    "#https://www2.commsec.com.au/quotes/announcements?stockCode=LKE&exchangeCode=ASX\n",
    "\n",
    "#this one also has announcements\n",
    "#https://www.marketindex.com.au/asx/tul\n",
    "\n",
    "#this is the site to use\n",
    "#https://www.asx.com.au/asx/v2/statistics/announcements.do?by=asxCode&asxCode=LKE&timeframe=Y&year=2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20053864",
   "metadata": {},
   "source": [
    "## GET THE LIST OF TICKER CODES ACTIVE ON THE ASX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "asx_url = \"https://www.marketindex.com.au/asx-listed-companies\"\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d752db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.asxlistedcompanies.com/'\n",
    "# Open the URL as Browser, not as python urllib\n",
    "page=urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "infile=urllib.request.urlopen(page).read()\n",
    "data = infile.decode('ISO-8859-1') # Read the content as string decoded with ISO-8859-1\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "asx_table = soup.find(\"table\",{\"class\":\"tableizer-table sortable\"})\n",
    "\n",
    "table_body = asx_table.find('tbody')\n",
    "rows = table_body.find_all('tr')\n",
    "ticker_codes = []\n",
    "\n",
    "for row in rows: #for each ticker code\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    ticker_code = ''.join(cols[0]) #turn list to string\n",
    "    ticker_codes.append(ticker_code) #aappend to list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "asx_table = soup.find(\"table\",{\"class\":\"tableizer-table sortable\"})\n",
    "#print(asx_table)\n",
    "\n",
    "table_body = asx_table.find('tbody')\n",
    "rows = table_body.find_all('tr')\n",
    "ticker_codes = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    ticker_code = ''.join(cols[0]) #turn list to string\n",
    "    ticker_codes.append(ticker_code)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6999dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ticker_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989d5bb",
   "metadata": {},
   "source": [
    "## NOW GET THE LIST OF PDFS FOR EACH TICKER DURING THE DEFINED TIME PERIOD\n",
    "## IN ORDER TO GET THE ACTUAL PDF THERE ARE TWO LINKS WE NEED\n",
    "## THIS ONE - AND THEN AFTER GOING TO THE URL WE NEED TO CLICK ON ONE MORE TO ACTUALLY DOWNLOAD THE PDF\n",
    "## SO THIS IS STEP 1 OF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c532b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "asx_home_page = 'https://www.asx.com.au'\n",
    "#'https://www.asx.com.au/asxpdf/20220729/pdf/45ccj9f9k1tgh4.pdf'\n",
    "base_url = 'https://www.asx.com.au/asx/v2/statistics/announcements.do?by=asxCode&asxCode='\n",
    "#then concatenate the below\n",
    "url_suffix = '&timeframe=D&period=M3'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": 'Mozilla/5.0',\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Postman-Token\": \"8eb5df70-4da6-4ba1-a9dd-e68880316cd9,30ac79fa-969b-4a24-8035-26ad1a2650e1\",\n",
    "    \"Host\": \"medianet.edmond-de-rothschild.fr\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"cache-control\": \"no-cache\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "full_ticker = []\n",
    "for ticker in ticker_codes:\n",
    "    time.sleep(5)\n",
    "    #then concatenate the ticker code\n",
    "    ticker_code = ticker\n",
    "    #then concatenate the below\n",
    "    url = base_url+ticker_code+url_suffix\n",
    "\n",
    "    # Open the URL as Browser, not as python urllib\n",
    "    page=urllib.request.Request(url,headers=headers) \n",
    "    infile=urllib.request.urlopen(page).read()\n",
    "    data = infile.decode('ISO-8859-1') # Read the content as string decoded with ISO-8859-1\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    try:\n",
    "        soupy = soup.findAll('table')[0].findAll('tr')\n",
    "        soupy2 = soup.findAll('tbody')\n",
    "\n",
    "        for row in soupy:\n",
    "            sublist = []\n",
    "\n",
    "            for a in row.find_all('a', href=True):\n",
    "                sublist.append(ticker_code)\n",
    "                link = a['href']\n",
    "                sublist.append(link)\n",
    "            for b in row.find_all('a'):\n",
    "                pdf_title = b.contents[0].strip()\n",
    "                sublist.append(pdf_title)\n",
    "            for c in row.find_all('td'): #date\n",
    "                try:\n",
    "                    date = c.contents[0].strip()  \n",
    "                    sublist.append(date)\n",
    "                except TypeError:\n",
    "                    print(\"date error for \",ticker_code,c.contents)\n",
    "            for d in row.find_all(\"td\",class_=\"pricesens\"): #market sensitive?\n",
    "                if str(d.contents)[3].lower() == \"n\":\n",
    "                    sens = 1\n",
    "                else:\n",
    "                    sens = 0\n",
    "                sublist.append(sens)\n",
    "            #pdf_list_ = [link,pdf_title,date,pricesens]\n",
    "            full_ticker.append(sublist)\n",
    "                    #print(\"dt:\",a.select_one(\"span[class='dates-time']\"))\n",
    "    except IndexError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame\n",
    "ticker_df = pd.DataFrame(full_ticker, columns = ['ticker', 'pdf_link', 'pdf_title','pdf_date','blank','blank1','market_sens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ec935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop blank rows\n",
    "#drop the two blank columns that somehow get created\n",
    "ticker_df = ticker_df[ticker_df['pdf_link'].notna()]\n",
    "ticker_df = ticker_df[ticker_df['pdf_date'].notna()]\n",
    "ticker_df.drop(['blank', 'blank1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export so we can save progress\n",
    "ticker_df.to_csv('full_ticker.csv', encoding='utf-8', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94060e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "ticker_df = pd.read_csv (r'full_ticker.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a4050",
   "metadata": {},
   "source": [
    "## NOW GET THE TRUE PDF LINK BY CLICKING ON THE LINK IN THE PREVIOUS STEP\n",
    "## AFTER OPENING THE TRUE LINK WE CAN DOWNLOAD THE PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b87a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe to list for easier processing\n",
    "ticker_df_sub = ticker_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the 'true' pdf link\n",
    "def get_pdf_link(url):\n",
    "    webpage = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'},timeout=5).text\n",
    "    soup_2 = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "    #this gets the pdf url\n",
    "    for tag in soup_2.find_all(\"input\", type=\"hidden\"):\n",
    "        pdf_link = tag[\"value\"]\n",
    "        pdf_link = asx_home_page+pdf_link\n",
    "    return pdf_link\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download the pdf from the 'true' pdf link\n",
    "loc = 'C:\\\\Users\\\\jarma\\\\ms_ds_a3\\\\pdfs\\\\'\n",
    "\n",
    "def dl_pdf(url,name):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    loc = 'C:\\\\Users\\\\jarma\\\\ms_ds_a3\\\\pdfs\\\\' + name\n",
    "    print(\"saving \" + name)\n",
    "    with open(loc, \"wb\") as pdf:\n",
    "        pdf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f231943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download pdfs\n",
    "pdf_filenames = []\n",
    "#cutting off at 12,000 pdfs\n",
    "for i in ticker_df_sub[0:12_000]: \n",
    "    try:\n",
    "        pdf_link = asx_home_page+i[1]\n",
    "        #not currently using this but keeping it in case it's needed later\n",
    "        #creates custom filename by concatenating ticker code and whether it's market sensitive\n",
    "        pdf_title = str(i[2])+str(i[0])\n",
    "        #print(pdf_link)\n",
    "        pdf_link_true = get_pdf_link(pdf_link)\n",
    "\n",
    "        #turn \"https://www.asx.com.au/asxpdf/20220810/pdf/45cpsctzl1mj8p.pdf\"\n",
    "        #into pdf/45cpsctzl1mj8p.pdf\n",
    "        pdf_filename = pdf_link_true.rsplit('/',1)[-1] \n",
    "        #print(pdf_filename)\n",
    "        #append the pdf name to a list so we know which company/date it's for    \n",
    "        intra_list = []  \n",
    "        #intra_list = [i[1],i[2],i[3],i[4],i[5],i[6],pdf_filename]\n",
    "        intra_list = [i[0],i[1],i[2],i[3],i[4],pdf_filename]\n",
    "        pdf_filenames.append(intra_list)  \n",
    "\n",
    "        #time.sleep(5) #have a nap so we don't trip up any ddos alerts     \n",
    "        #print(\"sleeping\")\n",
    "\n",
    "        pdf_loc = loc + pdf_filename\n",
    "        #print(\"pdf_loc = \",pdf_loc)\n",
    "        file = Path(pdf_loc)\n",
    "        if file.is_file():\n",
    "            print('pdf ' + pdf_filename +' exists, moving to next one')\n",
    "            continue\n",
    "        else:\n",
    "            print(\"don't have pdf - downloading: \" + pdf_filename) \n",
    "            dl_pdf(pdf_link_true,pdf_filename)\n",
    "\n",
    "    except: #leaving this open-ended because it turns out there are approx one million possible network errors\n",
    "        print(\"outer continue\")\n",
    "        continue\n",
    "print('done')        \n",
    "    \n",
    "    #pdf_filenames.append(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df = []\n",
    "for i in pdf_filenames:\n",
    "    pdf_df_intra = []\n",
    "    \n",
    "    new_date = i[3]\n",
    "    year = new_date[6:]\n",
    "    month = new_date[3:5]\n",
    "    \n",
    "    day = new_date[0:2]\n",
    "    yyyymmdd = year+month+day\n",
    "    pdf_df_intra = [i[0],i[2],i[3],i[4],i[5],yyyymmdd]\n",
    "    pdf_df.append(pdf_df_intra)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77935f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pdf_df[0:1]:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print(i[2])\n",
    "    print(i[3])\n",
    "    print(i[4])\n",
    "    print(i[5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec33033",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df_columns = ['ticker', 'pdf_title','pdf_date','market_sens','pdf_filename','yyyymmdd']\n",
    "# Create the pandas DataFrame\n",
    "pdf_df = pd.DataFrame(pdf_df, columns = pdf_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "pdf_df.to_csv('pdf_df.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "pdf_df = pd.read_csv (r'pdf_df.csv')\n",
    "pdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397dac9d",
   "metadata": {},
   "source": [
    "## NOW GET MARKET DATA FOR STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#should have done programatically, but was faster to create in excel as we need to remove weekends\n",
    "# date_list = ['20220708','20220711','20220712','20220713','20220714','20220715','20220718','20220719','20220720',\n",
    "#              '20220721','20220722','20220725','20220726','20220727','20220728','20220729','20220801','20220802',\n",
    "#              '20220803','20220804','20220805','20220808','20220809','20220810','20220811']\n",
    "\n",
    "#full 3 month\n",
    "date_list = ['20220407','20220408','20220411','20220412','20220413','20220414','20220415','20220418','20220419',\n",
    "             '20220420','20220421','20220422','20220425','20220426','20220427','20220428','20220429','20220502',\n",
    "             '20220503','20220504','20220505','20220506','20220509','20220510','20220511','20220512','20220513',\n",
    "             '20220516','20220517','20220518','20220519','20220520','20220523','20220524','20220525','20220526',\n",
    "             '20220527','20220530','20220531','20220601','20220602','20220603','20220606','20220607','20220608',\n",
    "             '20220609','20220610','20220613','20220614','20220615','20220616','20220617','20220620','20220621',\n",
    "             '20220622','20220623','20220624','20220627','20220628','20220629','20220630','20220701','20220704',\n",
    "             '20220705','20220706','20220707','20220708','20220711','20220712','20220713','20220714','20220715',\n",
    "             '20220718','20220719','20220720','20220721','20220722','20220725','20220726','20220727','20220728',\n",
    "             '20220729','20220801','20220802','20220803','20220804','20220805','20220808','20220809','20220810',\n",
    "             '20220811','20220812','20220815']\n",
    "\n",
    "dot_csv = \".csv\"\n",
    "\n",
    "url_list = [\n",
    "     'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220708.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023305Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=2cb415dd29e47f5521b736af6043aa92a7f7e1cee5d04153f26031d8ad6c9ae3'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220711.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023238Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=e0dbaa04873c33f8ad328a92ad52bfd90ac9d0348e5344105b0f4caeab358680'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220712.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023226Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=9afe5de897386ed415f9ab8fb610732e0ad3cf7fd77db236c1aba67ec9f0b95f'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220713.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023219Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=d78e0777c384eebb03277d0946f92d77559f51319208cf265d32f9200f179c6c'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220714.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023210Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=a4b98740dc33070d03fcc63c82506ea62e1241af607054accf1d348d5b31d2a2'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220715.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023200Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=36a81e2bac17cba27213438b30be7dca54bd2b4fb4aaba6b7f7e6a8af1223708'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220718.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023151Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=b51b0b979f9a1445cef5ba3c7a38005e1e97d29fa87c4d5b8a7f421a33e2da70'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220719.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023142Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=393b8ffc1a18f5a01a9dceb556f5bcf5a1a08f5d95e4e643267fb3c8582261b8'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220720.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023133Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=218471baad86e26689ed27ff12d465db11b171f1e80b8a798dc8791f62fb01a6'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220721.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023125Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=b5febbecf5ad8e3935e976854e69c5e1d41ca0383309f5f287b96f5355a2e88f'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220722.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023115Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=f7eb41a13fb583e44e0ba29128acfe559be131b461e639f5dd14ef906247ac47'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220725.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023059Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=bc316f4fceb6f09d595f913371c6ec21693f4b8e3c2f78646bc0072831576fbc'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220726.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023017Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=08cf1302bca27c944c68bb9fd863ebd7d733b1b997c5ac78882e2137231d9f10'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220727.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T023009Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=1e56e1a52ff43029987f3d94ad8fa1f06eb90ff7a524dcbbe81cd0aa2edad78c'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220728.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022957Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=5fdaf4fd0d52fee27d730c86982d1cc6e1ef958603fd224ed214bea902b33deb'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220729.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022947Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=6a715405dcc8470f72fd463195b5a72530f5cf6472ff606cdfe0387bc1083e01'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220801.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022937Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=bf3a5d3d46df5eddf8e88fc15dc16fdab67cdd48c4e7fe61d9a0bf613ce50634'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220802.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022925Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=b78560085ed80684014777626a5a33aa40666a85f8e91a779648274354bec30f'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220803.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022904Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=ec3ca2fc68db5b4e40c0f198978220c91b69ffa3a718fcef0fd6e2bd64d40377'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220804.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022854Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=f7fae547782e71738e88080529d00023d7e1aebc2341638d64e10e4266a17cd2'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220805.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022843Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=ae8cf1e1a8ed570f286a65e56261105801399a373181be5d747c44c466029da5'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220808.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022829Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=a5eac91d265059f8a6d8775d9afb35b4ed3fd2debd914a8cccd27ae030357ffa'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220809.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022808Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=7113973d3225228140b44e2ffe857edbb596e2777df6c86d6a10e6a583ceb3ba'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220810.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022430Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=3a290debe1e686c2b3dc167bb9647370a86e54f61902ba195d6e1081580b65f1'\n",
    "    ,'https://s3.ap-southeast-2.amazonaws.com/files.marketindex.com.au/asx-historical-data/20220811.csv?response-content-disposition=attachment&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUX2WTWTUAHLLG7FK%2F20220812%2Fap-southeast-2%2Fs3%2Faws4_request&X-Amz-Date=20220812T022252Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Signature=3874d04e2a649a80ab104434f3c020f8b8936c87b7dd9d8a799b3c8b471af6bc']\n",
    "    \n",
    "date_url = list(zip(date_list,url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loc = 'C:\\\\Users\\\\jarma\\\\ms_ds_a3\\\\csvs\\\\'\n",
    "def dl_csv(url,name):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    loc = 'C:\\\\Users\\\\jarma\\\\ms_ds_a3\\\\csvs\\\\' + name\n",
    "    print(\"saving \" + name)\n",
    "    with open(loc, \"wb\") as csv:\n",
    "        csv.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download each csv, name them to be yyyymmdd.csv\n",
    "#sourced from: https://www.marketindex.com.au/market-snapshots\n",
    "# for i in date_url:\n",
    "#     dl_csv(i[1],str(i[0])+dot_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the csvs, append them together\n",
    "li = []\n",
    "\n",
    "for i in date_list:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_loc+i+dot_csv, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b79d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort to create lagged variable for previous close day's price\n",
    "frame.sort_values(['Code', 'Date'], ascending=[True, False])\n",
    "frame['Data_lagged'] = (frame.sort_values(by=['Date'], ascending=True)\n",
    "                       .groupby(['Code'])['Close'].shift(1))\n",
    "frame.sort_values(['Code', 'Date'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calculate % change for current day compared to previous day close\n",
    "def perc_change(row):\n",
    "    xx = (1-(float(row.Data_lagged)/float(row.Close)))*100\n",
    "    return xx\n",
    "\n",
    "#assign the above\n",
    "frame['perc_chg'] = frame.apply(perc_change, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are missing the lagged values (2022 07 08)\n",
    "frame = frame[frame['Data_lagged'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e07a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "frame.to_csv('frame.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "frame = pd.read_csv (r'frame.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35373a",
   "metadata": {},
   "source": [
    "## NOW MERGE MARKET DATA WITH BASE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0974e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df['yyyymmdd'] = pdf_df['yyyymmdd'].astype(str)\n",
    "frame['Date'] = frame['Date'].astype(str)\n",
    "\n",
    "drop_frame_cols = ['Open','High','Low','Close','Volume','Data_lagged']\n",
    "thin_frame = frame.drop(drop_frame_cols, axis=1)\n",
    "\n",
    "merged = pd.merge(pdf_df, thin_frame,  how='left', left_on=['ticker','yyyymmdd'], right_on = ['Code','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_drop_cols = ['pdf_date','Code','Date']\n",
    "\n",
    "merged = merged.drop(merge_drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "merged.to_csv('merged.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e93926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "merged = pd.read_csv (r'merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 28 obs that are missing perc_chg due to trading halt; dropping these\n",
    "merged = merged[merged['perc_chg'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe to list for easier processing\n",
    "merged_li = merged.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d7d72",
   "metadata": {},
   "source": [
    "## NOW IMPORT PDFS AND CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_pdfs(pdf_inp,ticker):\n",
    "    #print(pdf_inp)\n",
    "    #pdf_loc = '\\\\ms_ds_a3\\\\pdfs\\\\' + pdf_inp\n",
    "    pdf_loc = 'C:\\\\Users\\\\jarma\\\\ms_ds_a3\\\\pdfs\\\\' + pdf_inp\n",
    "    raw = parser.from_file(pdf_loc)\n",
    "    content = raw['content'].replace(r'\\s+', ' ') \n",
    "\n",
    "    content = content.strip('\\\\n')\n",
    "    content = content.strip('\\t')\n",
    "\n",
    "    #need to do this a million times to successfully remove all those cheeky linebreaks \n",
    "    for i in range(90):\n",
    "        content = re.sub(r'\\n', ' ', content)\n",
    "    \n",
    "    #remove ticker code as it's not useful. we do it before turning things to lowercase to reduce false positive\n",
    "    content = re.sub(ticker, ' ', content)\n",
    "\n",
    "    #keep only upper/lower letters and spaces\n",
    "    pattern = re.compile('[^A-Za-z ]')\n",
    "    content = pattern.sub('',content)\n",
    "\n",
    "    #remove trailing spaces\n",
    "    content = content.strip()\n",
    "    \n",
    "    #turn all to lowercase\n",
    "    content = content.lower()\n",
    "    \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('www'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('http'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('mailto'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('asx'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.endswith('comau'))\n",
    "    #if it looks like a website, remove. should have a low false positive rate: https://wordfind.com/ends-with/com/\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.endswith('com'))    \n",
    "    #need to have startswith/endswith/equal to account for plural + some words are blendedlikethis\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.lower == \"announcement\")  \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('announcement'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.endswith('announcement'))\n",
    "    \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.lower == \"notification\")  \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('notification'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.endswith('notification'))    \n",
    "    \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.lower == \"market\")  \n",
    "    content = ' '.join(word for word in content.split(' ') if not word.startswith('market'))\n",
    "    content = ' '.join(word for word in content.split(' ') if not word.endswith('market'))      \n",
    "    \n",
    "    #remove words less than 4 chars long; all/and/etf/etc/a/to/we/an/it/the/for/of/at/is/on/has/had\n",
    "    content = re.sub(r'\\b\\w{1,3}\\b', '', content)    \n",
    "   \n",
    "    #remove consecutive identical words \n",
    "    content = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', content)\n",
    "\n",
    "   \n",
    "    #remove consecutive spaces\n",
    "    content = content.replace(r's+', ' ')     \n",
    "\n",
    "    #remove consecutive spaces/whitespace\n",
    "    content = re.sub(' +', ' ', content)\n",
    "\n",
    "    #remove trailing spaces\n",
    "    content = content.strip()\n",
    "\n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.lower == \"ordinary\")  \n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.startswith('ordinary'))\n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.endswith('ordinary'))        \n",
    "   \n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.lower == \"distribution\")  \n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.startswith('distribution'))\n",
    "#     content = ' '.join(word for word in content.split(' ') if not word.endswith('distribution'))      \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf = []\n",
    "\n",
    "for i in merged_li:\n",
    "    pdf_name = i[3]\n",
    "    clean_pdf = clean_pdfs(pdf_name,i[0])\n",
    "    #print(clean_pdf)\n",
    "    string_len = len(clean_pdf)\n",
    "    \n",
    "    \n",
    "    if i[5] > 1.00:\n",
    "        pos_neg_neut = 1\n",
    "        pnn_class = \"UP\"\n",
    "    elif i[5] < -1.00:\n",
    "        pos_neg_neut = -1\n",
    "        pnn_class = \"DOWN\"        \n",
    "    else:\n",
    "        pos_neg_neut = 0\n",
    "        pnn_class = \"SAME\"         \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    pdf_intra = [i[0],i[1],i[2],i[3],i[4],i[5],clean_pdf,string_len,pos_neg_neut,pnn_class]\n",
    "    merged_pdf.append(pdf_intra)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd06efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431db648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in merged_pdf[0:1]:\n",
    "    print(i[6],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d465fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now back to dataframe\n",
    "mergepdf_df_columns = ['ticker', 'pdf_title','market_sens','pdf_filename','yyyymmdd','perc_chg','clean_content','string_len','pos_neg_neut','pnn_class']\n",
    "# Create the pandas DataFrame\n",
    "merged_pdf_df = pd.DataFrame(merged_pdf, columns = mergepdf_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8822146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're dropping any rows where the length of the text is less than 600 characters (72 rows)\n",
    "#or greater than 18,000 as this is generally a sign of pdf being read incorrectly\n",
    "merged_pdf_df.drop(merged_pdf_df[merged_pdf_df.string_len < 600].index, inplace=True)\n",
    "merged_pdf_df.drop(merged_pdf_df[merged_pdf_df.string_len > 18000].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df.to_csv('merged_pdf_df.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "merged_pdf_df = pd.read_csv (r'merged_pdf_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f828f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort to create lagged variable for previous close day's price\n",
    "xx = merged_pdf_df.sort_values(['perc_chg'], ascending=[False])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df['clean_text_tok']=[nltk.word_tokenize(i) for i in merged_pdf_df['clean_content']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8235285",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "def lemm(x):\n",
    "    #Lemmatization\n",
    "    x = ' '.join([lemmer.lemmatize(w) for w in x.rstrip().split()])\n",
    "    return x\n",
    "\n",
    "def nouns(x):\n",
    "    tk = nltk.word_tokenize(x)\n",
    "    x = [word for (word, pos) in nltk.pos_tag(tk) if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')] \n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def stop_word_rm(x):\n",
    "    #remove stop words\n",
    "    f = lambda x: ' '.join([item for item in x.split() if item not in stopwords_eng])\n",
    "    stop = x.apply(f)\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99331e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df['clean_text_tok'] = stop_word_rm(merged_pdf_df['clean_content'])    # lemmatise every word.\n",
    "print(\"stopwords removed\")\n",
    "merged_pdf_df['clean_text_tok'] = merged_pdf_df['clean_text_tok'].apply(lemm)    # lemmatise every word.\n",
    "print(\"lemmatised\")\n",
    "\n",
    "#commenting out the nouns extraction as we're not using it and it takes forever\n",
    "#merged_pdf_df['nouns'] = merged_pdf_df['clean_content'].apply(nouns)    # lemmatise every word.\n",
    "#print(\"nouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2731a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to tokenize sentence\n",
    "merged_pdf_df['toked'] = merged_pdf_df.apply(lambda row: nltk.word_tokenize(row['clean_text_tok']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ba162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output and rename in case we need to rollback\n",
    "merged_pdf_df.to_csv('merged_pdf_df_clean.csv', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff: fast forward to this point\n",
    "merged_pdf_df = pd.read_csv (r'merged_pdf_df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa88389",
   "metadata": {},
   "source": [
    "## VISUALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "#print(merged_pdf_df.toked)\n",
    "#pd.options.display.max_colwidth = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be46585",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pdf_df.word2vec = Word2Vec(merged_pdf_df['toked'], min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(word2vec.wv.key_to_index)\n",
    "print(vocabulary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3606815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26929168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_text = merged_pdf_df['clean_text_tok'].values\n",
    "all_text = ' '.join(all_text)\n",
    "\n",
    "wordcloud = WordCloud(width=2560, height=1440, \n",
    "                    background_color='black',\n",
    "                    min_font_size=10)\n",
    "word_cloud = wordcloud.generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(word_cloud)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
